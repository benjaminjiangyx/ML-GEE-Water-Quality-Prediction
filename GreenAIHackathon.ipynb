{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl1pcLpFfOB7",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Install required packages\n",
        "!pip install earthengine-api geemap pandas numpy scikit-learn xgboost statsmodels plotly dash geopandas folium\n",
        "\n",
        "# Import libraries\n",
        "import ee\n",
        "import geemap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import xgboost as xgb\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import geopandas as gpd\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import json\n",
        "\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project = 'greenaihackathon-476122')\n",
        "\n",
        "print(\"✓ All packages installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Define SF Bay Area bounding box\n",
        "sf_bay_bounds = ee.Geometry.Rectangle([-122.6, 37.2, -121.5, 38.2])\n",
        "\n",
        "# Load watershed boundaries (HUC-8 or HUC-10)\n",
        "watersheds = ee.FeatureCollection(\"USGS/WBD/2017/HUC08\") \\\n",
        "    .filterBounds(sf_bay_bounds)\n",
        "\n",
        "# Visualize with geemap\n",
        "Map = geemap.Map(center=[37.8, -122.4], zoom=9)\n",
        "Map.addLayer(sf_bay_bounds, {'color': 'red'}, 'SF Bay Area')\n",
        "Map.addLayer(watersheds, {'color': 'blue'}, 'Watersheds')\n",
        "Map"
      ],
      "metadata": {
        "id": "rGcs2bPygL8N",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class GEEFeatureExtractor:\n",
        "    \"\"\"Extract environmental features from Google Earth Engine\"\"\"\n",
        "\n",
        "    def __init__(self, start_date='2020-01-01', end_date='2024-12-31'):\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date\n",
        "        self.sf_bay = ee.Geometry.Rectangle([-122.6, 37.2, -121.5, 38.2])\n",
        "\n",
        "    def get_land_cover_features(self, geometry, year=2021):\n",
        "        \"\"\"Extract land cover percentages from NLCD\"\"\"\n",
        "\n",
        "        # Load NLCD\n",
        "        nlcd = ee.ImageCollection('USGS/NLCD_RELEASES/2021_REL/NLCD') \\\n",
        "            .filterDate(f'{year}-01-01', f'{year}-12-31') \\\n",
        "            .first() \\\n",
        "            .select('landcover') \\\n",
        "            .clip(geometry)\n",
        "\n",
        "        # Define land cover classes\n",
        "        # 11: Open Water, 21-24: Developed, 41-43: Forest, 52: Shrub, 71: Grassland, 81-82: Agriculture, 90-95: Wetlands\n",
        "\n",
        "        # Calculate area for each class\n",
        "        area_image = ee.Image.pixelArea().addBands(nlcd)\n",
        "\n",
        "        areas = area_image.reduceRegion(\n",
        "            reducer=ee.Reducer.sum().group(\n",
        "                groupField=1,\n",
        "                groupName='landcover'\n",
        "            ),\n",
        "            geometry=geometry,\n",
        "            scale=30,\n",
        "            maxPixels=1e9\n",
        "        )\n",
        "\n",
        "        # Convert to dictionary\n",
        "        class_areas = areas.getInfo()['groups']\n",
        "\n",
        "        # Calculate percentages\n",
        "        total_area = sum([item['sum'] for item in class_areas])\n",
        "\n",
        "        features = {}\n",
        "        for item in class_areas:\n",
        "            lc_class = item['landcover']\n",
        "            pct = (item['sum'] / total_area) * 100\n",
        "\n",
        "            if lc_class in [21, 22, 23, 24]:  # Developed\n",
        "                features['developed_pct'] = features.get('developed_pct', 0) + pct\n",
        "            elif lc_class in [41, 42, 43]:  # Forest\n",
        "                features['forest_pct'] = features.get('forest_pct', 0) + pct\n",
        "            elif lc_class in [81, 82]:  # Agriculture\n",
        "                features['agriculture_pct'] = features.get('agriculture_pct', 0) + pct\n",
        "            elif lc_class == 11:  # Water\n",
        "                features['water_pct'] = pct\n",
        "            elif lc_class in [90, 95]:  # Wetlands\n",
        "                features['wetlands_pct'] = features.get('wetlands_pct', 0) + pct\n",
        "\n",
        "        return features\n",
        "\n",
        "    def get_impervious_surface(self, geometry):\n",
        "        \"\"\"Extract impervious surface percentage\"\"\"\n",
        "\n",
        "        impervious = ee.ImageCollection('USGS/NLCD_RELEASES/2021_REL/NLCD') \\\n",
        "            .first() \\\n",
        "            .select('impervious') \\\n",
        "            .clip(geometry)\n",
        "\n",
        "        mean_impervious = impervious.reduceRegion(\n",
        "            reducer=ee.Reducer.mean(),\n",
        "            geometry=geometry,\n",
        "            scale=30,\n",
        "            maxPixels=1e9\n",
        "        )\n",
        "\n",
        "        return {'impervious_pct': mean_impervious.getInfo()['impervious']}\n",
        "\n",
        "    def get_ndvi_stats(self, geometry, start_date, end_date):\n",
        "        \"\"\"Calculate NDVI statistics from Sentinel-2\"\"\"\n",
        "\n",
        "        def add_ndvi(image):\n",
        "            ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
        "            return image.addBands(ndvi)\n",
        "\n",
        "        # Load Sentinel-2 data\n",
        "        sentinel = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
        "            .filterBounds(geometry) \\\n",
        "            .filterDate(start_date, end_date) \\\n",
        "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n",
        "            .map(add_ndvi)\n",
        "\n",
        "        # Calculate mean NDVI\n",
        "        ndvi_mean = sentinel.select('NDVI').mean().clip(geometry)\n",
        "\n",
        "        stats = ndvi_mean.reduceRegion(\n",
        "            reducer=ee.Reducer.mean().combine(\n",
        "                reducer2=ee.Reducer.stdDev(),\n",
        "                sharedInputs=True\n",
        "            ),\n",
        "            geometry=geometry,\n",
        "            scale=100,\n",
        "            maxPixels=1e9\n",
        "        )\n",
        "\n",
        "        stats_dict = stats.getInfo()\n",
        "        return {\n",
        "            'ndvi_mean': stats_dict.get('NDVI_mean', 0),\n",
        "            'ndvi_std': stats_dict.get('NDVI_stdDev', 0)\n",
        "        }\n",
        "\n",
        "    def get_precipitation(self, geometry, start_date, end_date):\n",
        "        \"\"\"Get precipitation data from CHIRPS\"\"\"\n",
        "\n",
        "        precip = ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY') \\\n",
        "            .filterDate(start_date, end_date) \\\n",
        "            .filterBounds(geometry)\n",
        "\n",
        "        # Sum precipitation\n",
        "        total_precip = precip.sum().clip(geometry)\n",
        "\n",
        "        precip_stats = total_precip.reduceRegion(\n",
        "            reducer=ee.Reducer.mean(),\n",
        "            geometry=geometry,\n",
        "            scale=5000,\n",
        "            maxPixels=1e9\n",
        "        )\n",
        "\n",
        "        return {'precipitation_mm': precip_stats.getInfo()['precipitation']}\n",
        "\n",
        "    def get_temperature(self, geometry, start_date, end_date):\n",
        "        \"\"\"Get temperature data from MODIS\"\"\"\n",
        "\n",
        "        temp = ee.ImageCollection('MODIS/006/MOD11A2') \\\n",
        "            .filterDate(start_date, end_date) \\\n",
        "            .filterBounds(geometry) \\\n",
        "            .select('LST_Day_1km')\n",
        "\n",
        "        # Convert from Kelvin to Celsius\n",
        "        temp_celsius = temp.mean().multiply(0.02).subtract(273.15).clip(geometry)\n",
        "\n",
        "        temp_stats = temp_celsius.reduceRegion(\n",
        "            reducer=ee.Reducer.mean(),\n",
        "            geometry=geometry,\n",
        "            scale=1000,\n",
        "            maxPixels=1e9\n",
        "        )\n",
        "\n",
        "        return {'temperature_celsius': temp_stats.getInfo()['LST_Day_1km']}\n",
        "\n",
        "    def extract_all_features(self, geometry, date):\n",
        "        \"\"\"Extract all features for a given geometry and date\"\"\"\n",
        "\n",
        "        features = {}\n",
        "\n",
        "        # Date ranges\n",
        "        date_obj = pd.to_datetime(date)\n",
        "        end_date_str = date_obj.strftime('%Y-%m-%d')\n",
        "        start_date_30d = (date_obj - timedelta(days=30)).strftime('%Y-%m-%d')\n",
        "        start_date_90d = (date_obj - timedelta(days=90)).strftime('%Y-%m-%d')\n",
        "\n",
        "        print(f\"Extracting features for {date}...\")\n",
        "\n",
        "        # Land cover (static)\n",
        "        try:\n",
        "            features.update(self.get_land_cover_features(geometry))\n",
        "        except Exception as e:\n",
        "            print(f\"  Land cover error: {e}\")\n",
        "\n",
        "        # Impervious surface (static)\n",
        "        try:\n",
        "            features.update(self.get_impervious_surface(geometry))\n",
        "        except Exception as e:\n",
        "            print(f\"  Impervious surface error: {e}\")\n",
        "\n",
        "        # NDVI (30-day window)\n",
        "        try:\n",
        "            features.update(self.get_ndvi_stats(geometry, start_date_30d, end_date_str))\n",
        "        except Exception as e:\n",
        "            print(f\"  NDVI error: {e}\")\n",
        "\n",
        "        # Precipitation (30-day total)\n",
        "        try:\n",
        "            features.update(self.get_precipitation(geometry, start_date_30d, end_date_str))\n",
        "        except Exception as e:\n",
        "            print(f\"  Precipitation error: {e}\")\n",
        "\n",
        "        # Temperature (30-day average)\n",
        "        try:\n",
        "            features.update(self.get_temperature(geometry, start_date_30d, end_date_str))\n",
        "        except Exception as e:\n",
        "            print(f\"  Temperature error: {e}\")\n",
        "\n",
        "        # Add temporal features\n",
        "        features['month'] = date_obj.month\n",
        "        features['season'] = (date_obj.month % 12 + 3) // 3\n",
        "        features['year'] = date_obj.year\n",
        "        features['day_of_year'] = date_obj.dayofyear\n",
        "\n",
        "        return features\n",
        "\n",
        "# Initialize extractor\n",
        "extractor = GEEFeatureExtractor()\n",
        "print(\"✓ GEE Feature Extractor initialized!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pQPKjMxsg9j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def clean_ceden_data(ceden_data):\n",
        "    \"\"\"\n",
        "    Clean CEDEN water quality data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    ceden_data : pd.DataFrame\n",
        "        Raw CEDEN data from TSV file\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Cleaned data with standardized columns\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    # Make a copy to avoid modifying original\n",
        "    df = ceden_data.copy()\n",
        "\n",
        "    print(f\"Initial rows: {len(df)}\")\n",
        "\n",
        "    # 1. KEEP ONLY FIELD SAMPLES (remove QA samples)\n",
        "    # The 'isQA' column indicates QA samples\n",
        "    df = df[df['isQA'] == False].copy()\n",
        "    print(f\"After removing QA samples: {len(df)}\")\n",
        "\n",
        "    # 2. FILTER BY RESULT QUALIFIERS\n",
        "    # Remove non-detect (ND) and problematic qualifiers\n",
        "    # Keep: '=' (detected), 'DNQ' (detected but not quantified)\n",
        "    valid_qualifiers = ['=', 'DNQ', None, np.nan]\n",
        "    df = df[df['ResultQualCode'].isin(valid_qualifiers) | df['ResultQualCode'].isna()].copy()\n",
        "    print(f\"After filtering result qualifiers: {len(df)}\")\n",
        "\n",
        "    # 3. KEEP ONLY WATER SAMPLES\n",
        "    df = df[df['MatrixName'] == 'samplewater'].copy()\n",
        "    print(f\"After filtering to water samples: {len(df)}\")\n",
        "\n",
        "    # 4. STANDARDIZE DATE\n",
        "    df['SampleDate'] = pd.to_datetime(df['SampleDate'], errors='coerce')\n",
        "    df = df.dropna(subset=['SampleDate'])\n",
        "    print(f\"After date parsing: {len(df)}\")\n",
        "\n",
        "    # 5. ENSURE VALID COORDINATES\n",
        "    df = df.dropna(subset=['latitude', 'longitude'])\n",
        "    df = df[(df['latitude'] != 0) & (df['longitude'] != 0)]\n",
        "    print(f\"After coordinate validation: {len(df)}\")\n",
        "\n",
        "    # 6. CONVERT RESULT TO NUMERIC\n",
        "    df['Result'] = pd.to_numeric(df['Result'], errors='coerce')\n",
        "\n",
        "    # 7. REMOVE ROWS WITH MISSING CRITICAL VALUES\n",
        "    df = df.dropna(subset=['Result', 'Analyte', 'StationCode'])\n",
        "    print(f\"After removing missing critical values: {len(df)}\")\n",
        "\n",
        "    # 8. STANDARDIZE COLUMN NAMES (keep key columns)\n",
        "    columns_to_keep = {\n",
        "        'StationCode': 'station_id',\n",
        "        'StationName': 'station_name',\n",
        "        'SampleDate': 'date',\n",
        "        'Analyte': 'parameter',\n",
        "        'Result': 'value',\n",
        "        'Unit': 'unit',\n",
        "        'latitude': 'latitude',\n",
        "        'longitude': 'longitude',\n",
        "        'ResultQualCode': 'qualifier',\n",
        "        'CollectionDepth': 'depth_m',\n",
        "        'Program': 'program',\n",
        "        'county': 'county'\n",
        "    }\n",
        "\n",
        "    df = df.rename(columns=columns_to_keep)\n",
        "    df = df[[col for col in columns_to_keep.values() if col in df.columns]]\n",
        "\n",
        "    # 9. REMOVE DUPLICATES (keep first occurrence)\n",
        "    # Based on station, date, parameter\n",
        "    df = df.drop_duplicates(subset=['station_id', 'date', 'parameter'], keep='first')\n",
        "    print(f\"After removing duplicates: {len(df)}\")\n",
        "\n",
        "    # 10. SORT BY STATION AND DATE\n",
        "    df = df.sort_values(['station_id', 'date']).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\nFinal cleaned rows: {len(df)}\")\n",
        "    print(f\"Unique stations: {df['station_id'].nunique()}\")\n",
        "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "    print(f\"Parameters: {sorted(df['parameter'].unique())}\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "snm4iA58qARH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def transform_ceden_to_wide(ceden_clean):\n",
        "    \"\"\"\n",
        "    Transform cleaned CEDEN data from long to wide format.\n",
        "    Pivots so each parameter becomes a column.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    ceden_clean : pd.DataFrame\n",
        "        Cleaned CEDEN data from clean_ceden_data()\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Wide format with one row per station-date, parameters as columns\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    print(f\"Input rows: {len(ceden_clean)}\")\n",
        "    print(f\"Unique parameters: {ceden_clean['parameter'].nunique()}\")\n",
        "\n",
        "    # 1. CREATE STANDARDIZED PARAMETER NAMES (lowercase, underscores)\n",
        "    def standardize_parameter_name(param):\n",
        "        \"\"\"Convert parameter names to clean column names\"\"\"\n",
        "        # Remove special characters, convert to lowercase\n",
        "        clean = str(param).lower()\n",
        "        clean = clean.replace(',', '').replace('.', '')\n",
        "        clean = clean.replace('  ', ' ').strip()\n",
        "        clean = clean.replace(' ', '_')\n",
        "        clean = clean.replace('-', '_')\n",
        "        return clean\n",
        "\n",
        "    ceden_clean['parameter_clean'] = ceden_clean['parameter'].apply(standardize_parameter_name)\n",
        "\n",
        "    # 2. SHOW PARAMETER MAPPING\n",
        "    param_mapping = ceden_clean[['parameter', 'parameter_clean']].drop_duplicates()\n",
        "    print(\"\\nParameter name mapping:\")\n",
        "    for _, row in param_mapping.iterrows():\n",
        "        print(f\"  {row['parameter']} -> {row['parameter_clean']}\")\n",
        "\n",
        "    # 3. HANDLE MULTIPLE MEASUREMENTS ON SAME DATE\n",
        "    # Take mean if multiple values exist for same station-date-parameter\n",
        "    agg_df = ceden_clean.groupby(\n",
        "        ['station_id', 'station_name', 'date', 'latitude', 'longitude',\n",
        "         'parameter_clean', 'unit', 'county', 'program'],\n",
        "        dropna=False\n",
        "    ).agg({\n",
        "        'value': 'mean',  # Average if multiple samples\n",
        "        'qualifier': 'first'  # Keep first qualifier\n",
        "    }).reset_index()\n",
        "\n",
        "    print(f\"\\nAfter aggregation: {len(agg_df)} rows\")\n",
        "\n",
        "    # 4. ADD UNIT TO PARAMETER NAME (to avoid conflicts with different units)\n",
        "    # e.g., \"nitrogen_total_mg_l\" vs \"nitrogen_total_ug_l\"\n",
        "    def add_unit_to_param(row):\n",
        "        param = row['parameter_clean']\n",
        "        unit = str(row['unit']).lower() if pd.notna(row['unit']) else ''\n",
        "\n",
        "        # Clean unit string\n",
        "        unit = unit.replace('/', '_').replace(' ', '').replace('.', '')\n",
        "\n",
        "        if unit and unit != 'nan':\n",
        "            return f\"{param}_{unit}\"\n",
        "        return param\n",
        "\n",
        "    agg_df['parameter_with_unit'] = agg_df.apply(add_unit_to_param, axis=1)\n",
        "\n",
        "    # 5. PIVOT TO WIDE FORMAT\n",
        "    # Keep station metadata separate\n",
        "    metadata_cols = ['station_id', 'station_name', 'date', 'latitude', 'longitude', 'county', 'program']\n",
        "\n",
        "    wide_df = agg_df.pivot_table(\n",
        "        index=metadata_cols,\n",
        "        columns='parameter_with_unit',\n",
        "        values='value',\n",
        "        aggfunc='first'  # Safety in case there are still duplicates\n",
        "    ).reset_index()\n",
        "\n",
        "    # 6. CLEAN UP COLUMN NAMES\n",
        "    wide_df.columns.name = None  # Remove the 'parameter_with_unit' label\n",
        "\n",
        "    print(f\"\\nWide format created:\")\n",
        "    print(f\"  Rows (station-date combinations): {len(wide_df)}\")\n",
        "    print(f\"  Columns: {len(wide_df.columns)}\")\n",
        "    print(f\"  Parameter columns: {len(wide_df.columns) - len(metadata_cols)}\")\n",
        "\n",
        "    # 7. SHOW SAMPLE OF PARAMETER COLUMNS\n",
        "    param_cols = [col for col in wide_df.columns if col not in metadata_cols]\n",
        "    print(f\"\\nParameter columns created:\")\n",
        "    for col in sorted(param_cols)[:20]:  # Show first 20\n",
        "        non_null = wide_df[col].notna().sum()\n",
        "        print(f\"  {col}: {non_null} non-null values\")\n",
        "    if len(param_cols) > 20:\n",
        "        print(f\"  ... and {len(param_cols) - 20} more\")\n",
        "\n",
        "    # 8. SORT BY STATION AND DATE\n",
        "    wide_df = wide_df.sort_values(['station_id', 'date']).reset_index(drop=True)\n",
        "\n",
        "    return wide_df\n",
        "\"\"\"\n",
        "What this function does:\n",
        "\n",
        "1. **Standardizes parameter names**: Converts \"Chlorophyll a, Total\" → \"chlorophyll_a_total\"\n",
        "2. **Handles duplicates**: Averages multiple measurements on same date\n",
        "3. **Adds units to parameter names**: \"nitrogen_total_mg/L\" becomes \"nitrogen_total_mg_l\" to distinguish from \"nitrogen_total_ug_l\"\n",
        "4. **Pivots to wide format**: Each parameter becomes its own column\n",
        "5. **Preserves metadata**: Station ID, name, coordinates, date stay as index columns\n",
        "\n",
        "**Expected output structure:**\n",
        "\n",
        "station_id | date       | latitude | longitude | chlorophyll_a_total_mg_m3 | nitrogen_total_mg_l | ...\n",
        "-----------|------------|----------|-----------|---------------------------|---------------------|----\n",
        "205COY222  | 2020-09-14 | 37.289   | -121.765  | NaN                       | NaN                 | ...\n",
        "205COY182  | 2020-09-14 | 37.358   | -121.859  | 5.3                       | NaN                 | ...\n",
        "\"\"\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "s9ep7uspqIVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def handle_ceden_issues(wq_data):\n",
        "    \"\"\"\n",
        "    Handle common data quality issues in wide-format CEDEN data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    wq_data : pd.DataFrame\n",
        "        Wide-format water quality data from transform_ceden_to_wide()\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Cleaned data with issues resolved\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    df = wq_data.copy()\n",
        "\n",
        "    print(f\"Initial shape: {df.shape}\")\n",
        "    print(f\"Initial columns: {len(df.columns)}\")\n",
        "\n",
        "    # Identify metadata vs parameter columns\n",
        "    metadata_cols = ['station_id', 'station_name', 'date', 'latitude', 'longitude', 'county', 'program']\n",
        "    param_cols = [col for col in df.columns if col not in metadata_cols]\n",
        "\n",
        "    print(f\"Parameter columns: {len(param_cols)}\")\n",
        "\n",
        "    # 1. HANDLE NEGATIVE VALUES (concentrations can't be negative)\n",
        "    print(\"\\n1. Handling negative values...\")\n",
        "    negative_counts = {}\n",
        "    for col in param_cols:\n",
        "        if df[col].dtype in ['float64', 'int64']:\n",
        "            neg_count = (df[col] < 0).sum()\n",
        "            if neg_count > 0:\n",
        "                negative_counts[col] = neg_count\n",
        "                df.loc[df[col] < 0, col] = np.nan\n",
        "\n",
        "    if negative_counts:\n",
        "        print(f\"   Set {sum(negative_counts.values())} negative values to NaN across {len(negative_counts)} parameters\")\n",
        "        for col, count in list(negative_counts.items())[:5]:\n",
        "            print(f\"     {col}: {count} negative values\")\n",
        "        if len(negative_counts) > 5:\n",
        "            print(f\"     ... and {len(negative_counts) - 5} more parameters\")\n",
        "    else:\n",
        "        print(\"   No negative values found\")\n",
        "\n",
        "    # 2. DROP COLUMNS WITH >95% MISSING DATA\n",
        "    print(\"\\n2. Dropping sparse columns (>95% missing)...\")\n",
        "    missing_threshold = 0.95\n",
        "    cols_to_drop = []\n",
        "\n",
        "    for col in param_cols:\n",
        "        missing_pct = df[col].isna().sum() / len(df)\n",
        "        if missing_pct > missing_threshold:\n",
        "            cols_to_drop.append(col)\n",
        "\n",
        "    if cols_to_drop:\n",
        "        print(f\"   Dropping {len(cols_to_drop)} sparse columns:\")\n",
        "        for col in cols_to_drop[:10]:\n",
        "            missing_pct = df[col].isna().sum() / len(df) * 100\n",
        "            print(f\"     {col}: {missing_pct:.1f}% missing\")\n",
        "        if len(cols_to_drop) > 10:\n",
        "            print(f\"     ... and {len(cols_to_drop) - 10} more\")\n",
        "\n",
        "        df = df.drop(columns=cols_to_drop)\n",
        "        param_cols = [col for col in param_cols if col not in cols_to_drop]\n",
        "    else:\n",
        "        print(\"   No sparse columns to drop\")\n",
        "\n",
        "    # 3. REMOVE DUPLICATE ROWS (safety check)\n",
        "    print(\"\\n3. Checking for duplicate rows...\")\n",
        "    initial_rows = len(df)\n",
        "    df = df.drop_duplicates(subset=['station_id', 'date'], keep='first')\n",
        "    removed = initial_rows - len(df)\n",
        "    if removed > 0:\n",
        "        print(f\"   Removed {removed} duplicate station-date combinations\")\n",
        "    else:\n",
        "        print(\"   No duplicates found\")\n",
        "\n",
        "    # 4. HANDLE EXTREME OUTLIERS (>99.9th percentile or <0.1th percentile)\n",
        "    # Only flag, don't remove - let user decide\n",
        "    print(\"\\n4. Detecting extreme outliers...\")\n",
        "    outlier_summary = {}\n",
        "\n",
        "    for col in param_cols:\n",
        "        if df[col].dtype in ['float64', 'int64'] and df[col].notna().sum() > 10:\n",
        "            q_low = df[col].quantile(0.001)\n",
        "            q_high = df[col].quantile(0.999)\n",
        "\n",
        "            outliers = ((df[col] < q_low) | (df[col] > q_high)) & df[col].notna()\n",
        "            outlier_count = outliers.sum()\n",
        "\n",
        "            if outlier_count > 0:\n",
        "                outlier_summary[col] = {\n",
        "                    'count': outlier_count,\n",
        "                    'range': (q_low, q_high),\n",
        "                    'extreme_values': df.loc[outliers, col].tolist()[:3]\n",
        "                }\n",
        "\n",
        "    if outlier_summary:\n",
        "        print(f\"   Found extreme outliers in {len(outlier_summary)} parameters\")\n",
        "        for col, info in list(outlier_summary.items())[:5]:\n",
        "            print(f\"     {col}: {info['count']} outliers (normal range: {info['range'][0]:.2f} - {info['range'][1]:.2f})\")\n",
        "        if len(outlier_summary) > 5:\n",
        "            print(f\"     ... and {len(outlier_summary) - 5} more parameters\")\n",
        "        print(\"   Note: Outliers retained in dataset (review before modeling)\")\n",
        "    else:\n",
        "        print(\"   No extreme outliers detected\")\n",
        "\n",
        "    # 5. ENSURE PROPER DATA TYPES\n",
        "    print(\"\\n5. Standardizing data types...\")\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
        "    df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
        "\n",
        "    for col in param_cols:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    print(\"   Data types standardized\")\n",
        "\n",
        "    # 6. REMOVE ROWS WITH MISSING COORDINATES\n",
        "    print(\"\\n6. Removing rows with invalid coordinates...\")\n",
        "    initial_rows = len(df)\n",
        "    df = df.dropna(subset=['latitude', 'longitude'])\n",
        "    removed = initial_rows - len(df)\n",
        "    if removed > 0:\n",
        "        print(f\"   Removed {removed} rows with missing coordinates\")\n",
        "    else:\n",
        "        print(\"   All rows have valid coordinates\")\n",
        "\n",
        "    # 7. SUMMARY STATISTICS\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL DATASET SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
        "    print(f\"Unique stations: {df['station_id'].nunique()}\")\n",
        "    print(f\"Parameter columns: {len(param_cols)}\")\n",
        "\n",
        "    # Show completeness of remaining parameters\n",
        "    print(\"\\nParameter completeness (non-null %):\")\n",
        "    param_completeness = []\n",
        "    for col in param_cols:\n",
        "        completeness = (1 - df[col].isna().sum() / len(df)) * 100\n",
        "        param_completeness.append((col, completeness))\n",
        "\n",
        "    param_completeness.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    for col, pct in param_completeness[:10]:\n",
        "        print(f\"  {col}: {pct:.1f}%\")\n",
        "    if len(param_completeness) > 10:\n",
        "        print(f\"  ... and {len(param_completeness) - 10} more parameters\")\n",
        "\n",
        "    # Reset index\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sITvsUI4qlGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6232bb1a",
        "collapsed": true
      },
      "source": [
        "def extract_features_for_stations(wq_data, extractor, sample_size=20):\n",
        "    \"\"\"Extract GEE features for each station and date\"\"\"\n",
        "\n",
        "    # Sample data for demonstration (remove sampling for full analysis)\n",
        "    wq_sample = wq_data.sample(min(sample_size, len(wq_data)), random_state=42)\n",
        "\n",
        "    all_features = []\n",
        "\n",
        "    for idx, row in wq_sample.iterrows():\n",
        "        print(f\"Processing {idx+1}/{len(wq_sample)}...\")\n",
        "\n",
        "        # Create point geometry for station\n",
        "        point = ee.Geometry.Point([row['longitude'], row['latitude']])\n",
        "\n",
        "        # Create buffer (e.g., 5km radius)\n",
        "        buffer = point.buffer(5000)\n",
        "\n",
        "        # Extract features\n",
        "        try:\n",
        "            features = extractor.extract_all_features(buffer, row['date'])\n",
        "            features['station_id'] = row['station_id']\n",
        "            features['date'] = row['date']\n",
        "            features['latitude'] = row['latitude']\n",
        "            features['longitude'] = row['longitude']\n",
        "            # Use .loc[] for more robust access\n",
        "            # FIX: Correct column name for nitrogen and phosphorus based on transform_ceden_to_wide output\n",
        "            features['nitrogen_mg_l'] = row.loc['nitrogen_total_kjeldahl_total_mg_l'] if 'nitrogen_total_kjeldahl_total_mg_l' in row.index else np.nan\n",
        "            features['phosphorus_mg_l'] = row.loc['phosphorus_as_p_total_mg_l'] if 'phosphorus_as_p_total_mg_l' in row.index else np.nan\n",
        "\n",
        "            all_features.append(features)\n",
        "        except Exception as e:\n",
        "            print(f\"  Error processing row {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    df_features = pd.DataFrame(all_features)\n",
        "    return df_features\n",
        "\n",
        "# Extract features (this will take some time!)\n",
        "print(\"Extracting GEE features for water quality stations...\")\n",
        "print(\"Note: Using sample of 50 measurements for demonstration\")\n",
        "print(\"Remove sample_size parameter for full dataset\")\n",
        "\n",
        "df_features = extract_features_for_stations(wq_data_clean, extractor, sample_size=50) # Use wq_data_clean as input\n",
        "\n",
        "# Save to CSV\n",
        "df_features.to_csv('sf_bay_features.csv', index=False)\n",
        "print(f\"✓ Extracted features for {len(df_features)} measurements\")\n",
        "print(df_features.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def engineer_features(df_features):\n",
        "    \"\"\"\n",
        "    Engineer features for water quality + GEE dataset.\n",
        "\n",
        "    Steps:\n",
        "    - Impute missing numerical and categorical values\n",
        "    - Encode categorical variables\n",
        "    - Scale numerical features\n",
        "    - Add derived temporal features\n",
        "    - Return engineered dataframe ready for modeling\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "    df = df_features.copy()\n",
        "\n",
        "    # --- 1. Missing value handling ---\n",
        "    num_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    # Exclude identifier columns from engineering\n",
        "    exclude_cols = ['station_id', 'latitude', 'longitude', 'date']\n",
        "    num_cols = [col for col in num_cols if col not in exclude_cols]\n",
        "    cat_cols = [col for col in cat_cols if col not in exclude_cols]\n",
        "\n",
        "    # Fill missing numeric values with mean\n",
        "    for col in num_cols:\n",
        "        df[col] = df[col].fillna(df[col].mean())\n",
        "\n",
        "    # Fill missing categorical values with mode\n",
        "    for col in cat_cols:\n",
        "        df[col] = df[col].fillna(df[col].mode()[0] if df[col].mode().size > 0 else 'missing')\n",
        "\n",
        "    # --- 2. Temporal feature engineering ---\n",
        "    # Convert date to datetime if not already\n",
        "    if 'date' in df.columns and not np.issubdtype(df['date'].dtype, np.datetime64):\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    if 'date' in df.columns:\n",
        "        df['day_of_week'] = df['date'].dt.dayofweek\n",
        "        df['month'] = df['date'].dt.month\n",
        "        df['day_of_year'] = df['date'].dt.dayofyear\n",
        "\n",
        "    # --- 3. Categorical encoding ---\n",
        "    # One-hot encode categorical columns (e.g., season)\n",
        "    if cat_cols:\n",
        "        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "        encoded = encoder.fit_transform(df[cat_cols])\n",
        "        encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(cat_cols), index=df.index)\n",
        "        df = pd.concat([df.drop(columns=cat_cols), encoded_df], axis=1)\n",
        "\n",
        "    # --- 4. Scaling numeric features ---\n",
        "    scaler = StandardScaler()\n",
        "    scaled = scaler.fit_transform(df[num_cols])\n",
        "    scaled_df = pd.DataFrame(scaled, columns=[f\"{col}_scaled\" for col in num_cols], index=df.index)\n",
        "    df = pd.concat([df, scaled_df], axis=1)\n",
        "\n",
        "    # --- 5. Feature interactions (simple example) ---\n",
        "    if 'ndvi_mean' in df.columns and 'precipitation_mm' in df.columns:\n",
        "        df['ndvi_precip_interaction'] = df['ndvi_mean'] * df['precipitation_mm']\n",
        "\n",
        "    # Drop original lat/lon identifiers if desired\n",
        "    # df = df.drop(columns=['latitude', 'longitude'])\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tQ84imq5sApf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class WaterQualityPredictor:\n",
        "    \"\"\"\n",
        "    End-to-end water quality prediction using Google Earth Engine features.\n",
        "\n",
        "    Supports multiple models and provides comprehensive evaluation metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, target_variable='dissolved_oxygen', model_type='random_forest'):\n",
        "        \"\"\"\n",
        "        Initialize the predictor.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        target_variable : str\n",
        "            Name of the water quality parameter to predict\n",
        "        model_type : str\n",
        "            Type of model to use: 'random_forest', 'gradient_boosting', 'linear', 'xgboost'\n",
        "        \"\"\"\n",
        "        self.target_variable = target_variable\n",
        "        self.model_type = model_type\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.feature_names = None\n",
        "        self.feature_importances = None\n",
        "        self.metrics = {}\n",
        "\n",
        "        # Initialize model based on type\n",
        "        self._initialize_model()\n",
        "\n",
        "    def _initialize_model(self):\n",
        "        \"\"\"Initialize the ML model based on model_type.\"\"\"\n",
        "        from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "        from sklearn.linear_model import LinearRegression, Ridge\n",
        "\n",
        "        if self.model_type == 'random_forest':\n",
        "            self.model = RandomForestRegressor(\n",
        "                n_estimators=100,\n",
        "                max_depth=10,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "        elif self.model_type == 'gradient_boosting':\n",
        "            self.model = GradientBoostingRegressor(\n",
        "                n_estimators=100,\n",
        "                max_depth=5,\n",
        "                learning_rate=0.1,\n",
        "                random_state=42\n",
        "            )\n",
        "        elif self.model_type == 'linear':\n",
        "            self.model = Ridge(alpha=1.0, random_state=42)\n",
        "        elif self.model_type == 'xgboost':\n",
        "            try:\n",
        "                import xgboost as xgb\n",
        "                self.model = xgb.XGBRegressor(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=6,\n",
        "                    learning_rate=0.1,\n",
        "                    random_state=42\n",
        "                )\n",
        "            except ImportError:\n",
        "                print(\"XGBoost not installed. Falling back to Random Forest.\")\n",
        "                self.model_type = 'random_forest'\n",
        "                self._initialize_model()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model_type: {self.model_type}\")\n",
        "\n",
        "    def prepare_data(self, df_engineered, test_size=0.2, random_state=42):\n",
        "        \"\"\"\n",
        "        Split data into train/test sets and separate features from target.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df_engineered : pd.DataFrame\n",
        "            Engineered features DataFrame\n",
        "        test_size : float\n",
        "            Proportion of data for testing\n",
        "        random_state : int\n",
        "            Random seed for reproducibility\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple : (X_train, X_test, y_train, y_test)\n",
        "        \"\"\"\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        import pandas as pd\n",
        "        import numpy as np\n",
        "\n",
        "        df = df_engineered.copy()\n",
        "\n",
        "        # Check if target exists\n",
        "        if self.target_variable not in df.columns:\n",
        "            raise ValueError(f\"Target variable '{self.target_variable}' not found in dataframe\")\n",
        "\n",
        "        # Remove rows with missing target values\n",
        "        df_clean = df.dropna(subset=[self.target_variable]).copy()\n",
        "        print(f\"Rows with valid {self.target_variable}: {len(df_clean)}/{len(df)}\")\n",
        "\n",
        "        # Define feature columns (exclude identifiers and target)\n",
        "        exclude_cols = [\n",
        "            'station_id', 'latitude', 'longitude', 'date',\n",
        "            self.target_variable\n",
        "        ]\n",
        "\n",
        "        # Also exclude other water quality parameters if predicting one\n",
        "        wq_params = [\n",
        "            'dissolved_oxygen', 'ph', 'temperature', 'turbidity',\n",
        "            'conductivity', 'nitrate', 'phosphate', 'chlorophyll_a'\n",
        "        ]\n",
        "        for param in wq_params:\n",
        "            if param != self.target_variable and param in df_clean.columns:\n",
        "                exclude_cols.append(param)\n",
        "\n",
        "        feature_cols = [col for col in df_clean.columns if col not in exclude_cols]\n",
        "\n",
        "        # Separate features and target\n",
        "        X = df_clean[feature_cols].copy()\n",
        "        y = df_clean[self.target_variable].copy()\n",
        "\n",
        "        # Store feature names\n",
        "        self.feature_names = feature_cols\n",
        "\n",
        "        # Handle any remaining missing values in features\n",
        "        X = X.fillna(X.mean())\n",
        "\n",
        "        print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "        print(f\"Target variable: {self.target_variable}\")\n",
        "        print(f\"Number of features: {len(feature_cols)}\")\n",
        "        print(f\"Target range: [{y.min():.2f}, {y.max():.2f}]\")\n",
        "\n",
        "        # Split into train/test\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=random_state\n",
        "        )\n",
        "\n",
        "        print(f\"\\nTrain set size: {len(X_train)}\")\n",
        "        print(f\"Test set size: {len(X_test)}\")\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Train the model.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_train : pd.DataFrame or np.array\n",
        "            Training features\n",
        "        y_train : pd.Series or np.array\n",
        "            Training target values\n",
        "        \"\"\"\n",
        "        import time\n",
        "\n",
        "        print(f\"\\nTraining {self.model_type} model...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"✓ Training complete in {elapsed:.2f} seconds\")\n",
        "\n",
        "        # Extract feature importances if available\n",
        "        if hasattr(self.model, 'feature_importances_'):\n",
        "            self.feature_importances = pd.DataFrame({\n",
        "                'feature': self.feature_names,\n",
        "                'importance': self.model.feature_importances_\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluate model performance on test set.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_test : pd.DataFrame or np.array\n",
        "            Test features\n",
        "        y_test : pd.Series or np.array\n",
        "            Test target values\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict : Dictionary of evaluation metrics\n",
        "        \"\"\"\n",
        "        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "        import numpy as np\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = self.model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Mean Absolute Percentage Error\n",
        "        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "        self.metrics = {\n",
        "            'rmse': rmse,\n",
        "            'mae': mae,\n",
        "            'r2': r2,\n",
        "            'mape': mape,\n",
        "            'mse': mse\n",
        "        }\n",
        "\n",
        "        # Print results\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"MODEL EVALUATION - {self.target_variable}\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"Model Type: {self.model_type}\")\n",
        "        print(f\"\\nPerformance Metrics:\")\n",
        "        print(f\"  R² Score:  {r2:.4f}\")\n",
        "        print(f\"  RMSE:      {rmse:.4f}\")\n",
        "        print(f\"  MAE:       {mae:.4f}\")\n",
        "        print(f\"  MAPE:      {mape:.2f}%\")\n",
        "        print(f\"{'='*50}\\n\")\n",
        "\n",
        "        return self.metrics\n",
        "\n",
        "    def plot_results(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Create visualization plots for model performance.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_test : pd.DataFrame or np.array\n",
        "            Test features\n",
        "        y_test : pd.Series or np.array\n",
        "            Test target values\n",
        "        \"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "\n",
        "        y_pred = self.model.predict(X_test)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "        # Plot 1: Predicted vs Actual\n",
        "        ax1 = axes[0]\n",
        "        ax1.scatter(y_test, y_pred, alpha=0.5, edgecolors='k', linewidth=0.5)\n",
        "\n",
        "        # Perfect prediction line\n",
        "        min_val = min(y_test.min(), y_pred.min())\n",
        "        max_val = max(y_test.max(), y_pred.max())\n",
        "        ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
        "\n",
        "        ax1.set_xlabel(f'Actual {self.target_variable}', fontsize=12)\n",
        "        ax1.set_ylabel(f'Predicted {self.target_variable}', fontsize=12)\n",
        "        ax1.set_title(f'Predicted vs Actual\\nR² = {self.metrics[\"r2\"]:.4f}', fontsize=14)\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: Residuals\n",
        "        ax2 = axes[1]\n",
        "        residuals = y_test - y_pred\n",
        "        ax2.scatter(y_pred, residuals, alpha=0.5, edgecolors='k', linewidth=0.5)\n",
        "        ax2.axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "        ax2.set_xlabel(f'Predicted {self.target_variable}', fontsize=12)\n",
        "        ax2.set_ylabel('Residuals', fontsize=12)\n",
        "        ax2.set_title('Residual Plot', fontsize=14)\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 3: Feature Importances (top 15)\n",
        "        ax3 = axes[2]\n",
        "        if self.feature_importances is not None:\n",
        "            top_features = self.feature_importances.head(15)\n",
        "            ax3.barh(range(len(top_features)), top_features['importance'])\n",
        "            ax3.set_yticks(range(len(top_features)))\n",
        "            ax3.set_yticklabels(top_features['feature'], fontsize=10)\n",
        "            ax3.set_xlabel('Importance', fontsize=12)\n",
        "            ax3.set_title('Top 15 Feature Importances', fontsize=14)\n",
        "            ax3.invert_yaxis()\n",
        "        else:\n",
        "            ax3.text(0.5, 0.5, 'Feature importances\\nnot available\\nfor this model type',\n",
        "                    ha='center', va='center', fontsize=12)\n",
        "            ax3.set_xlim(0, 1)\n",
        "            ax3.set_ylim(0, 1)\n",
        "            ax3.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions on new data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pd.DataFrame or np.array\n",
        "            Features for prediction\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        np.array : Predicted values\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained yet. Call train() first.\")\n",
        "\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def get_feature_importance(self, top_n=20):\n",
        "        \"\"\"\n",
        "        Get top N most important features.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        top_n : int\n",
        "            Number of top features to return\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame : Top N features with importance scores\n",
        "        \"\"\"\n",
        "        if self.feature_importances is None:\n",
        "            print(\"Feature importances not available for this model type.\")\n",
        "            return None\n",
        "\n",
        "        return self.feature_importances.head(top_n)\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"Save trained model to disk.\"\"\"\n",
        "        import pickle\n",
        "\n",
        "        model_data = {\n",
        "            'model': self.model,\n",
        "            'target_variable': self.target_variable,\n",
        "            'model_type': self.model_type,\n",
        "            'feature_names': self.feature_names,\n",
        "            'feature_importances': self.feature_importances,\n",
        "            'metrics': self.metrics\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(model_data, f)\n",
        "\n",
        "        print(f\"✓ Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"Load trained model from disk.\"\"\"\n",
        "        import pickle\n",
        "\n",
        "        with open(filepath, 'rb') as f:\n",
        "            model_data = pickle.load(f)\n",
        "\n",
        "        self.model = model_data['model']\n",
        "        self.target_variable = model_data['target_variable']\n",
        "        self.model_type = model_data['model_type']\n",
        "        self.feature_names = model_data['feature_names']\n",
        "        self.feature_importances = model_data['feature_importances']\n",
        "        self.metrics = model_data['metrics']\n",
        "\n",
        "        print(f\"✓ Model loaded from {filepath}\")\n",
        "        print(f\"  Target: {self.target_variable}\")\n",
        "        print(f\"  Model type: {self.model_type}\")\n",
        "        print(f\"  R² score: {self.metrics.get('r2', 'N/A')}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "d6PrNxhSs41U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# COMPLETE PIPELINE: From CEDEN TSV to ML-ready features\n",
        "\n",
        "# 1. Upload and load CEDEN data\n",
        "print(\"STEP 1: Upload CEDEN data\")\n",
        "from google.colab import files # Import files here\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "ceden_data = pd.read_csv(filename, sep='\\t', encoding='utf-8', low_memory=False)\n",
        "\n",
        "# 2. Clean and prepare\n",
        "print(\"\\nSTEP 2: Clean data\")\n",
        "ceden_clean = clean_ceden_data(ceden_data)\n",
        "\n",
        "# 3. Transform to wide format\n",
        "print(\"\\nSTEP 3: Transform to wide format\")\n",
        "wq_data = transform_ceden_to_wide(ceden_clean)\n",
        "\n",
        "# 4. Handle issues\n",
        "print(\"\\nSTEP 4: Handle data issues\")\n",
        "wq_data_clean = handle_ceden_issues(wq_data)\n",
        "\n",
        "# 5. Extract GEE features\n",
        "print(\"\\nSTEP 5: Extract GEE features\")\n",
        "df_features = extract_features_for_stations(\n",
        "    wq_data_clean,\n",
        "    extractor,\n",
        "    sample_size=50  # Start with 50 for testing\n",
        ")\n",
        "\n",
        "# 6. Engineer additional features\n",
        "print(\"\\nSTEP 6: Engineer features\")\n",
        "df_engineered = engineer_features(df_features) # Moved from cell 584c7ea0\n",
        "\n",
        "# 7. Save results\n",
        "df_engineered.to_csv('sf_bay_ml_ready.csv', index=False)\n",
        "print(\"\\n✓ ML-ready dataset saved to sf_bay_ml_ready.csv\")\n",
        "print(f\"Final dataset: {len(df_engineered)} rows, {len(df_engineered.columns)} columns\")\n",
        "\n",
        "# 8. Train models\n",
        "print(\"\\nSTEP 7: Train ML models\")\n",
        "\n",
        "# --- FIX 1: Correctly initialize the predictor ---\n",
        "# 'target_variable' was misnamed 'target'\n",
        "# 'df_engineered' is passed to 'prepare_data', not the constructor.\n",
        "predictor = WaterQualityPredictor(\n",
        "    target_variable='nitrogen_mg_l',\n",
        "    model_type='random_forest'  # This is the model that will be trained\n",
        ")\n",
        "\n",
        "# --- FIX 2: Call methods correctly ---\n",
        "# 'prepare_data' needs the dataframe and returns the train/test splits.\n",
        "X_train, X_test, y_train, y_test = predictor.prepare_data(df_engineered)\n",
        "\n",
        "# Call the generic 'train' method\n",
        "predictor.train(X_train, y_train)\n",
        "\n",
        "# Call 'evaluate' and 'plot_results'\n",
        "print(\"\\nSTEP 8: Evaluate Random Forest model\")\n",
        "predictor.evaluate(X_test, y_test)\n",
        "predictor.plot_results(X_test, y_test)\n",
        "\n",
        "# --- (Optional) To compare models, create a second predictor ---\n",
        "print(\"\\n--- Training a second model (XGBoost) for comparison ---\")\n",
        "xgb_predictor = WaterQualityPredictor(\n",
        "    target_variable='nitrogen_mg_l',\n",
        "    model_type='xgboost'\n",
        ")\n",
        "# Re-use the same data splits\n",
        "xgb_predictor.train(X_train, y_train)\n",
        "# FIX: Manually set feature_names for xgb_predictor since prepare_data was not called on it\n",
        "xgb_predictor.feature_names = X_train.columns.tolist()\n",
        "print(\"\\nSTEP 9: Evaluate XGBoost model\")\n",
        "xgb_predictor.evaluate(X_test, y_test)\n",
        "xgb_predictor.plot_results(X_test, y_test)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OBobr8jTk-Ky",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a6e54d3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import timedelta\n",
        "\n",
        "# Get unique station IDs and their last known coordinates from df_engineered\n",
        "station_info = df_engineered[['station_id', 'latitude', 'longitude']].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Determine the start date for predictions (one year after the last date in the current data)\n",
        "latest_date = df_engineered['date'].max()\n",
        "prediction_start_date = latest_date + timedelta(days=365) - timedelta(days=latest_date.dayofyear)\n",
        "prediction_end_date = prediction_start_date + timedelta(days=364) # End of the next year\n",
        "\n",
        "# Generate future dates (e.g., first of each month for the next year)\n",
        "future_dates = pd.date_range(start=prediction_start_date.replace(day=1), end=prediction_end_date.replace(day=1), freq='MS')\n",
        "\n",
        "# Create a DataFrame for future predictions\n",
        "future_predictions_data = []\n",
        "\n",
        "# Get the mean of GEE features from the existing engineered data\n",
        "# Exclude 'station_id', 'date', 'latitude', 'longitude', 'nitrogen_mg_l', 'phosphorus_mg_l', and engineered temporal features for averaging\n",
        "\n",
        "# Identify numerical GEE features (excluding target and identifiers)\n",
        "gee_feature_cols = [col for col in df_engineered.columns if col not in ['station_id', 'date', 'latitude', 'longitude', 'nitrogen_mg_l', 'phosphorus_mg_l', 'month', 'season', 'year', 'day_of_year', 'day_of_week', 'ndvi_precip_interaction'] and 'scaled' not in col]\n",
        "\n",
        "# Calculate the mean of these GEE features from the existing data\n",
        "mean_gee_features = df_engineered[gee_feature_cols].mean().to_dict()\n",
        "\n",
        "for _, station_row in station_info.iterrows():\n",
        "    for future_date in future_dates:\n",
        "        row_data = {\n",
        "            'station_id': station_row['station_id'],\n",
        "            'latitude': station_row['latitude'],\n",
        "            'longitude': station_row['longitude'],\n",
        "            'date': future_date,\n",
        "            # Assign placeholder for target variables\n",
        "            'nitrogen_mg_l': np.nan,\n",
        "            'phosphorus_mg_l': np.nan,\n",
        "        }\n",
        "        # Add mean GEE features\n",
        "        row_data.update(mean_gee_features)\n",
        "        future_predictions_data.append(row_data)\n",
        "\n",
        "df_future_raw = pd.DataFrame(future_predictions_data)\n",
        "print(f\"Generated {len(df_future_raw)} future prediction points.\")\n",
        "print(\"First 5 rows of raw future prediction data:\")\n",
        "display(df_future_raw.head())\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}